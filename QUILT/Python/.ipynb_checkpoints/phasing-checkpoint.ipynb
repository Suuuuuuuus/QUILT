{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "import json\n",
    "import secrets\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import resource\n",
    "import pandas as pd\n",
    "# import sqlite3\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# from plotnine import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "# from collections import Counter\n",
    "# import seaborn as sns\n",
    "# import matplotlib.colors as mcolors\n",
    "# from matplotlib.ticker import FuncFormatter\n",
    "import itertools\n",
    "import collections\n",
    "import pyreadr\n",
    "# import patchworklib as pw\n",
    "# sys.path.append('/well/band/users/rbx225/software/lcwgsus/')\n",
    "# import lcwgsus\n",
    "# from lcwgsus.variables import *\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Genuine logics changed from the main script were marked by string **X** in the original R files.\n",
    "# Simplicity changes were marked by string **Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'DRB1'\n",
    "region = 'DRB1'\n",
    "nucleotides = ['A', 'T', 'C', 'G']\n",
    "ipd_gen_file = '/Users/sus_zhang/Desktop/Suuuuuuuus/misc_data/alignments/' + gene + '_gen.txt'\n",
    "if not os.path.exists(ipd_gen_file):\n",
    "    ipd_gen_file = '/well/band/users/rbx225/recyclable_files/hla/alignments/' + gene + '_gen.txt'\n",
    "\n",
    "hla_gene_information = pd.read_csv('/well/band/users/rbx225/software/QUILT_sus/hla_ancillary_files/hlagenes.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ipd_gen_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "gDNA_idx = []\n",
    "names = []\n",
    "i = 0\n",
    "while len(gDNA_idx) < 2:\n",
    "    l = lines[i]\n",
    "    if 'gDNA' in l:\n",
    "        gDNA_idx.append(i)\n",
    "    elif l.lstrip(' ').split(' ')[0].startswith(gene + '*'):\n",
    "        name = l.lstrip(' ').split(' ')[0]\n",
    "        names.append(name)\n",
    "    i += 1\n",
    "    \n",
    "first_base = int(lines[gDNA_idx[0]].split(' ')[-1].split('\\n')[0])\n",
    "n_alleles = gDNA_idx[1] - gDNA_idx[0] - 3\n",
    "\n",
    "alleles_dict = {k:'' for k in names}\n",
    "for i, s in enumerate(lines):\n",
    "    r = s.lstrip(' ')\n",
    "    if r.startswith(gene):\n",
    "        r = r.rstrip(' \\n')\n",
    "        name = r.split(' ')[0]\n",
    "        sequence = r.split(' ')[2:]\n",
    "        sequence = ''.join(sequence)\n",
    "        alleles_dict[name] = alleles_dict[name] + sequence\n",
    "    \n",
    "df = pd.DataFrame({key: list(value) for key, value in alleles_dict.items()}).T\n",
    "df = df.drop(columns=df.columns[df.eq('|').all()])\n",
    "df.columns = range(df.shape[1])\n",
    "\n",
    "length = len(df.columns)\n",
    "positions = [first_base]*length\n",
    "\n",
    "df.loc[len(df)] = positions\n",
    "\n",
    "r_idx = len(df) - 1\n",
    "for i in df.columns[1:]:\n",
    "    if df.iloc[0, i] == '.':\n",
    "        df.iloc[r_idx, i] = df.iloc[r_idx, i-1]\n",
    "    else:\n",
    "        if df.iloc[r_idx, i-1] != -1:\n",
    "            df.iloc[r_idx, i] = df.iloc[r_idx, i-1] + 1\n",
    "        else:\n",
    "            df.iloc[r_idx, i] = df.iloc[r_idx, i-1] + 2 # No zero in position. ATG is encoded by 1.\n",
    "df.columns = df.iloc[r_idx]\n",
    "df = df.iloc[:r_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_refdir = '/well/band/users/rbx225/GAMCC/results/hla/imputation/ref_panel/QUILT_ref_files/'\n",
    "auxiliary_dir = '/well/band/users/rbx225/GAMCC/results/hla/imputation/ref_panel/auxiliary_files/' \n",
    "all_haplotypes = pyreadr.read_r(original_refdir + 'quilt.hrc.hla.all.haplotypes.RData') # See if it is possible to remove the 32 bases dependency\n",
    "snpformatalleles = pyreadr.read_r(original_refdir + 'hlaDRB1snpformatalleles.RData')\n",
    "fullallelesfilledin = pyreadr.read_r(original_refdir + 'HLADRB1fullallelesfilledin.RData')\n",
    "full = pyreadr.read_r(original_refdir + 'hlaDRB1full.RData')\n",
    "\n",
    "hlatypes = pd.read_csv(auxiliary_dir + '20181129_HLA_types_full_1000_Genomes_Project_panel.txt', sep = '\\t')\n",
    "ref_samples = pd.read_csv(auxiliary_dir + 'oneKG.samples', sep = ' ')\n",
    "ref_samples_removed = ref_samples[~ref_samples['SAMPLE'].isin(hlatypes['Sample ID'].tolist())]\n",
    "samples_to_remove = ref_samples_removed['SAMPLE'].tolist()\n",
    "reference_exclude_samplelist_file = ''\n",
    "\n",
    "if reference_exclude_samplelist_file != '':\n",
    "    samples_to_remove = samples_to_remove + lcwgsus.read_tsv_as_lst(reference_exclude_samplelist_file)\n",
    "# if samples_to_remove is not none, save to \"hlauntyped.exclude.txt\"\n",
    "\n",
    "hlatypes = hlatypes[~hlatypes['Sample ID'].isin(samples_to_remove)]\n",
    "reference_samples = all_haplotypes['reference_samples']\n",
    "hlatypes = hlatypes[hlatypes['Sample ID'].isin(reference_samples['SAMPLE'].tolist())].reset_index(drop = True).sort_values(by = 'Sample ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oldsnpinfo -> prepared_vcf\n",
    "# ss -> variant_alleles\n",
    "# samples -> alleles\n",
    "# qq -> positions\n",
    "knownvarsfiltered = snpformatalleles['knownvarsfiltered'].copy()\n",
    "resmat = snpformatalleles['resmat'].copy()\n",
    "knownvarsfiltered = knownvarsfiltered[knownvarsfiltered[1].isin(nucleotides) & knownvarsfiltered[2].isin(nucleotides)].reset_index(drop = True)\n",
    "\n",
    "ids = \"chr6:\" + knownvarsfiltered[0].astype(str)\n",
    "prepared_vcf = pd.concat([ids, knownvarsfiltered], axis=1)\n",
    "alleles = resmat.index\n",
    "prepared_vcf.columns = [\"id\", \"position\", \"a0\", \"a1\"]\n",
    "prepared_vcf['position']  = prepared_vcf['position'].astype(int)\n",
    "\n",
    "ourpos = fullallelesfilledin['ourpos']['ourpos'].tolist()\n",
    "fullalleles = fullallelesfilledin['fullalleles'].copy()\n",
    "\n",
    "retained_index = []\n",
    "variant_alleles = pd.DataFrame(index=fullalleles.index)\n",
    "\n",
    "for i in range(len(prepared_vcf)):\n",
    "    _, pos, ref, alt = prepared_vcf.iloc[i,:]\n",
    "    index = np.where(ourpos == pos)[0]\n",
    "    if index.size != 0:\n",
    "        index = index[0]\n",
    "        retained_index.append(index)\n",
    "        variant_alleles[variant_alleles.shape[1]] = fullalleles[index].map({ref: '0', alt: '1'}).fillna(fullalleles[index])\n",
    "    else:\n",
    "        prepared_vcf.iloc[i,2] = 'tbr' # tbr: To be removed\n",
    "\n",
    "prepared_vcf = prepared_vcf[prepared_vcf['a0'] != 'tbr'].reset_index(drop = True)\n",
    "variant_alleles = variant_alleles.T\n",
    "\n",
    "positions = prepared_vcf['position'].astype(float).values  # Convert position to double\n",
    "\n",
    "zz = np.zeros(variant_alleles.shape[0]) \n",
    "for i in range(len(zz)):\n",
    "    zz[i] = np.sum(ourpos == positions[i])\n",
    "zz2 = np.zeros_like(zz)\n",
    "zz2 = np.sum(~((variant_alleles == '0') | (variant_alleles == '1')), axis=1)\n",
    "zz3 = np.zeros_like(zz)\n",
    "for i in range(len(zz3)):\n",
    "    zz3[i] = np.sum(positions == positions[i])\n",
    "zz4 = np.sum(~((variant_alleles == '0') | (variant_alleles == '1') | (variant_alleles == \".\")), axis=1)\n",
    "filter_conditions = (zz3 == 1) & (zz == 1) & (zz2 < variant_alleles.shape[1] * 0.1) & (zz2 < 0.5 * np.sum(variant_alleles == '1', axis=1)) & (zz4 == 0)\n",
    "\n",
    "# keep sites uniquely mapping, not overlapping another SNP, with at most 10% gaps and 2-fold more non-ancestral than gaps\n",
    "snpinfo = prepared_vcf.loc[filter_conditions, :].reset_index(drop = True)\n",
    "variant_alleles = variant_alleles.loc[filter_conditions, :]\n",
    "variant_alleles[variant_alleles == \".\"] = 0 # indels set to 0\n",
    "haps = variant_alleles.copy().reset_index(drop = True)\n",
    "# haps is `n_variant \\times n_alleles` df, with each entry as 0/1 representing ref/alt from the fake vcf \"prepared_vcf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array(alleles).reshape(-1, 1)\n",
    "c1 = [s[0].split('*') for s in samples]\n",
    "vv = np.empty((samples.shape[0], 3), dtype=object)\n",
    "ww = [c[1] for c in c1]\n",
    "for i in range(len(c1)):\n",
    "    vv[i, 0] = c1[i][0]\n",
    "c1 = [w.split(':') for w in ww]\n",
    "for i in range(len(c1)):\n",
    "    vv[i, 1] = c1[i][0]\n",
    "    vv[i, 2] = c1[i][1]\n",
    "fourdigit = [f\"{vv[i, 1]}:{vv[i, 2]}\" for i in range(len(vv))]\n",
    "ufourdigit = np.unique(fourdigit)\n",
    "# Convert reference alleles to 4-digit resolution. vv and ww are just intermediate variables of no use\n",
    "# ufourdigit is unique 4-digit alleles\n",
    "\n",
    "haps = np.array(haps, dtype=float)\n",
    "newhaps = np.empty((haps.shape[0], len(ufourdigit)))\n",
    "for i in range(newhaps.shape[1]):\n",
    "    tt = np.where(np.array(fourdigit) == ufourdigit[i])[0]\n",
    "    if len(tt) < 2:\n",
    "        tt = np.concatenate((tt, tt))\n",
    "    newhaps[:, i] = np.mean(haps[:, tt], axis=1)\n",
    "newhaps = pd.DataFrame(newhaps, columns = ufourdigit)\n",
    "# newhaps is the result after averaging 4 digit resolution\n",
    "    \n",
    "# pos is from the QUILT_prepare_ref utility. It seems to record biallelic SNP variant information\n",
    "pos = all_haplotypes['pos']\n",
    "\n",
    "cols = snpinfo.columns.tolist()\n",
    "full_vcf = pos.copy()[['POS', 'REF', 'ALT']]\n",
    "full_vcf.columns = cols[1:]\n",
    "same = pd.merge(full_vcf, snpinfo, on = cols[1:], how = 'inner')\n",
    "\n",
    "snpinfo_flipped = snpinfo.iloc[:,[0, 1, 3, 2]]\n",
    "snpinfo_flipped.columns = cols\n",
    "flipped = pd.merge(full_vcf, snpinfo_flipped, on = cols[1:], how = 'inner')\n",
    "\n",
    "merge = pd.concat([same, flipped]).sort_values(by = 'position', ascending = True)\n",
    "locs = full_vcf.index[full_vcf['position'].isin(merge['position'].tolist())]\n",
    "\n",
    "newhaps2 = newhaps.loc[newhaps.index.isin(snpinfo[snpinfo['position'].isin(same['position'])].index)]\n",
    "if len(flipped) != 0:\n",
    "    newhaps2_flipped = 1 - newhaps.loc[newhaps.index.isin(snpinfo[snpinfo['position'].isin(flipped['position'])].index)]\n",
    "    newhaps2 = pd.concat([newhaps2, newhaps2_flipped])\n",
    "newhaps2 = newhaps2.reset_index(drop = True)\n",
    "# newhaps2 further filtered out variants that are not observed in the 'pos' dataframe\n",
    "# This step really filters out a plethora of variants.. Is this really necessary? \n",
    "# Is it possible to circumvent that step such that we dont utilise any information from the `QUILT` output?\n",
    "\n",
    "# Or it might be those variants are what we could observe from the BAM files, if so that makes sense then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(matchlist):\n",
    "    qq = np.array(matchlist)\n",
    "    signs = np.zeros(len(matchlist), dtype=int)\n",
    "    \n",
    "    signs[qq < 0] = 1\n",
    "    qq[signs == 1] += 2**31\n",
    "    \n",
    "    res = np.zeros((len(matchlist), 0), dtype=float)\n",
    "    \n",
    "    for i in range(31):\n",
    "        tempres = qq % 2\n",
    "        qq = (qq - tempres) // 2\n",
    "        res = np.hstack((res, tempres.reshape(-1, 1)))\n",
    "    \n",
    "    res = np.hstack((res, signs.reshape(-1, 1)))\n",
    "    \n",
    "    res[res == 1] = 0.999\n",
    "    res[res == 0] = 0.001\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ourtypes -> reftypes\n",
    "region_pattern = f\"HLA.{region}.\"\n",
    "cols = [i for i, col in enumerate(hlatypes.columns) if re.search(region_pattern, col)]\n",
    "\n",
    "reftypes1 = hlatypes.iloc[:, cols[0]].astype(str).tolist()\n",
    "reftypes1 = [re.sub(r\"\\*\", \"\", t) for t in reftypes1]\n",
    "reftypes1 = np.array(reftypes1)\n",
    "vv = [t.split('/') for t in reftypes1]\n",
    "for i in range(len(reftypes1)):\n",
    "    reftypes1[i] = vv[i][0]\n",
    "\n",
    "reftypes2 = hlatypes.iloc[:, cols[1]].astype(str).tolist()\n",
    "reftypes2 = [re.sub(r\"\\*\", \"\", t) for t in reftypes2]\n",
    "\n",
    "vv = [t.split('/') for t in reftypes2]\n",
    "for i in range(len(reftypes2)):\n",
    "    reftypes2[i] = vv[i][0]\n",
    "reftypes2 = np.array(reftypes2)\n",
    "# vv is assigning the first hla type when it is ambiguous. What about remove those? **Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "start0 = pos.index[pos['POS'] == merge['position'].tolist()[0]][0]\n",
    "end0 = pos.index[pos['POS'] == merge['position'].tolist()[-1]][0]\n",
    "start32 = (start0 // 32) + 1  # per-chunk start\n",
    "end32 = (end0 // 32) + 1\n",
    "start = (start32 - 1) * 32 + 1\n",
    "end = end32 * 32\n",
    "\n",
    "distinctHapsIE = all_haplotypes['distinctHapsIE']\n",
    "distinctHapsB = all_haplotypes['distinctHapsB']\n",
    "rhb_t = all_haplotypes['rhb_t']\n",
    "tempIE = distinctHapsIE.iloc[:, start-1:end]\n",
    "tempB = distinctHapsB.iloc[:, start32-1:end32]\n",
    "temp = rhb_t.iloc[:, start32-1:end32]\n",
    "\n",
    "cc = np.concatenate((np.full(31, 0.001), [0.999]))\n",
    "temp2 = pd.DataFrame(np.empty((temp.shape[0], tempIE.shape[1])))\n",
    "for i in range(temp.shape[1]):\n",
    "    start_col = (i * 32)\n",
    "    end_col = (i + 1) * 32\n",
    "    for r, j in enumerate(temp.iloc[:, i].tolist()):\n",
    "        idx = tempB.iloc[:, i].index[tempB.iloc[:, i] == j]\n",
    "        if len(idx) != 0: # **Y**\n",
    "            temp2.iloc[r, start_col:end_col] = tempIE.iloc[idx[0], start_col:end_col] \n",
    "        \n",
    "        \n",
    "        # Handle NA (None) cases in temp\n",
    "    cc2 = np.where(pd.isna(temp.iloc[:, i]))[0]\n",
    "    if len(cc2) > 0:\n",
    "        cc3 = np.tile(cc, (len(cc2), 1))\n",
    "        temp2.iloc[cc2, start_col:end_col] = cc3\n",
    "\n",
    "    # Final step: Handle NA in temp2 for the final column of the block\n",
    "    needed = np.where(temp2.iloc[:, end_col-1] == 0)[0]\n",
    "    if len(needed) > 0:\n",
    "        temp2.iloc[needed, start_col:end_col] = translate(temp.iloc[needed, i])\n",
    "# temp2 seemes to contain haplotype (or actually ref/alt) information on a per-SNP rather than per-chunk base. felt like this bit can be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlocs = locs - start + 1\n",
    "hrchapstomatch = temp2.loc[:, newlocs].reset_index(drop = True)\n",
    "\n",
    "hrcfirstalleles = hrchapstomatch.iloc[::2, :].reset_index(drop = True)\n",
    "hrcfirstalleles.columns = range(hrcfirstalleles.shape[1])\n",
    "hrcsecondalleles = hrchapstomatch.iloc[1::2, :].reset_index(drop = True)\n",
    "hrcsecondalleles.columns = range(hrcsecondalleles.shape[1])\n",
    "\n",
    "temp = np.full(len(reftypes1), -1)\n",
    "temp[np.isin(reftypes1, newhaps2.columns)] = 1\n",
    "predfirstalleles = np.empty((len(temp), newhaps2.shape[0]))\n",
    "predfirstalleles[temp == 1, :] = newhaps2.loc[:, reftypes1[temp == 1]].T\n",
    "\n",
    "temp = np.full(len(reftypes2), -1)\n",
    "temp[np.isin(reftypes2, newhaps2.columns)] = 1\n",
    "predsecondalleles = np.empty((len(temp), newhaps2.shape[0]))\n",
    "predsecondalleles[temp == 1, :] = newhaps2.loc[:, reftypes2[temp == 1]].T\n",
    "\n",
    "#pred1st/2ndalleles is variant dosage observed from database\n",
    "\n",
    "dist11 = np.sum(np.abs(hrcfirstalleles - predfirstalleles), axis=1)\n",
    "dist12 = np.sum(np.abs(hrcfirstalleles - predsecondalleles), axis=1)\n",
    "dist21 = np.sum(np.abs(hrcsecondalleles - predfirstalleles), axis=1)\n",
    "dist22 = np.sum(np.abs(hrcsecondalleles - predsecondalleles), axis=1)\n",
    "\n",
    "w1 = np.zeros(len(dist11))\n",
    "w1[dist11 > dist12] = 1\n",
    "w1[dist11 < dist12] = -1\n",
    "\n",
    "w2 = np.zeros(len(dist21))\n",
    "w2[dist22 < dist21] = 1\n",
    "w2[dist22 > dist21] = -1\n",
    "\n",
    "# w1 and w2 assigns haplotypes based on pairwise distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Step 1: Calculate observed and predicted genotypes\n",
    "obsgen = hrcfirstalleles + hrcsecondalleles\n",
    "predgen = predfirstalleles + predsecondalleles\n",
    "\n",
    "# Step 2: Calculate correlations column-wise\n",
    "corr = np.empty(obsgen.shape[1])\n",
    "for i in range(obsgen.shape[1]):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        corr[i] = pearsonr(obsgen[:, i], predgen[:, i])[0] if not np.all(np.isnan(predgen[:, i])) else np.nan\n",
    "\n",
    "# Step 3: Filter columns with correlation > 0.8 and non-NA values\n",
    "valid_corr_indices = np.where((corr > 0.8) & (~np.isnan(corr)))[0]\n",
    "\n",
    "# Step 4: Compute distances for the filtered columns\n",
    "dist11 = np.sum(np.abs(hrcfirstalleles[:, valid_corr_indices] - predfirstalleles[:, valid_corr_indices]), axis=1)\n",
    "dist12 = np.sum(np.abs(hrcfirstalleles[:, valid_corr_indices] - predsecondalleles[:, valid_corr_indices]), axis=1)\n",
    "dist21 = np.sum(np.abs(hrcsecondalleles[:, valid_corr_indices] - predfirstalleles[:, valid_corr_indices]), axis=1)\n",
    "dist22 = np.sum(np.abs(hrcsecondalleles[:, valid_corr_indices] - predsecondalleles[:, valid_corr_indices]), axis=1)\n",
    "\n",
    "# Step 5: Calculate w1 and w2 based on distances\n",
    "w = np.zeros(len(dist11))\n",
    "w[dist11 < dist12] = 1\n",
    "w[dist11 > dist12] = -1\n",
    "w1 = w\n",
    "\n",
    "w = np.zeros(len(dist21))\n",
    "w[dist22 < dist21] = 1\n",
    "w[dist22 > dist21] = -1\n",
    "w2 = w\n",
    "\n",
    "# Print the contingency table of w1 and w2\n",
    "# We can use numpy's `np.histogram2d()` to create a table equivalent to `table()`\n",
    "w1_w2_table = np.histogram2d(w1, w2, bins=[[-1, 0, 1], [-1, 0, 1]])[0]\n",
    "\n",
    "# Step 6: Calculate phasing metrics\n",
    "phase1 = dist11 + dist22\n",
    "phase2 = dist12 + dist21\n",
    "\n",
    "# Step 7: Criteria for phasing\n",
    "d11 = dist11\n",
    "d21 = dist21\n",
    "d12 = dist12\n",
    "d22 = dist22\n",
    "\n",
    "phased = (\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 < 4) & (phase2 > 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 > 4) & (phase2 < 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 - phase2 > 2)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase2 - phase1 > 2)) |\n",
    "    (~np.isnan(ourtypes1) & ~np.isnan(ourtypes2) & (ourtypes1 == ourtypes2)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d22 - d12 > 2)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d12 - d22 > 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d11 - d21 > 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d21 - d11 > 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>position</th>\n",
       "      <th>a0</th>\n",
       "      <th>a1</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32579015</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>chr6:32579015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32579017</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>chr6:32579017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32579059</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>chr6:32579059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32579157</td>\n",
       "      <td>C</td>\n",
       "      <td>A</td>\n",
       "      <td>chr6:32579157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32579162</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>chr6:32579162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>32589394</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>chr6:32589394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>32589480</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>chr6:32589480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>32589515</td>\n",
       "      <td>T</td>\n",
       "      <td>C</td>\n",
       "      <td>chr6:32589515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>32589544</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>chr6:32589544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>32589551</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>chr6:32589551</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     position a0 a1             id\n",
       "0    32579015  A  G  chr6:32579015\n",
       "1    32579017  C  G  chr6:32579017\n",
       "2    32579059  A  T  chr6:32579059\n",
       "3    32579157  C  A  chr6:32579157\n",
       "4    32579162  C  G  chr6:32579162\n",
       "..        ... .. ..            ...\n",
       "352  32589394  G  A  chr6:32589394\n",
       "353  32589480  A  C  chr6:32589480\n",
       "354  32589515  T  C  chr6:32589515\n",
       "355  32589544  C  T  chr6:32589544\n",
       "356  32589551  A  C  chr6:32589551\n",
       "\n",
       "[357 rows x 4 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define phased1 and phased2 based on conditions\n",
    "phased1 = (\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 < 4) & (phase2 > 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase2 - phase1 > 2) & (phase1 < 4)) |\n",
    "    (~np.isnan(ourtypes1) & ~np.isnan(ourtypes2) & (ourtypes1 == ourtypes2)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d12 - d22 > 2) & (d22 < 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d21 - d11 > 2) & (d11 < 2))\n",
    ")\n",
    "\n",
    "phased2 = (\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 > 4) & (phase2 < 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 - phase2 > 2) & (phase2 < 4)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d22 - d12 > 2) & (d12 < 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d11 - d21 > 2) & (d21 < 2))\n",
    ")\n",
    "\n",
    "# Step 2: Assign alleles based on phased1 and phased2\n",
    "allele1 = np.full(len(ourtypes1), np.nan)\n",
    "allele2 = np.full(len(ourtypes1), np.nan)\n",
    "allele1[phased1] = ourtypes1[phased1]\n",
    "allele2[phased1] = ourtypes2[phased1]\n",
    "allele1[phased2] = ourtypes2[phased2]\n",
    "allele2[phased2] = ourtypes1[phased2]\n",
    "\n",
    "# Step 3: Iterate over extension sizes and refine phasing\n",
    "phased1old = phased1.copy()\n",
    "phased2old = phased2.copy()\n",
    "oldphase1 = phase1.copy()\n",
    "oldphase2 = phase2.copy()\n",
    "\n",
    "for extension in range(50, 1001, 50):\n",
    "    plt.scatter(oldphase1, oldphase2, c=1 + phased1.astype(int) + 2 * phased2.astype(int))\n",
    "    plt.show()\n",
    "\n",
    "    startandend = np.array([locs.min(), locs.max()])\n",
    "    startandend[0] -= extension\n",
    "    startandend[1] += extension\n",
    "    start32 = startandend[0] // 32 + 1\n",
    "    end32 = startandend[1] // 32 + 1\n",
    "    start = (start32 - 1) * 32 + 1\n",
    "    end = end32 * 32\n",
    "\n",
    "    tempIE = distinctHapsIE[:, start:end]\n",
    "    tempB = distinctHapsB[:, start32:end32]\n",
    "    temp = rhb_t[:, start32:end32]\n",
    "    temp2 = np.empty((temp.shape[0], tempIE.shape[1]))\n",
    "\n",
    "    cc = np.array([0.001] * 31 + [0.999])\n",
    "    \n",
    "    for i in range(temp.shape[1]):\n",
    "        temp2[:, i * 32:(i + 1) * 32] = tempIE[np.searchsorted(tempB[:, i], temp[:, i]), i * 32:(i + 1) * 32]\n",
    "        \n",
    "        cc2 = np.where(np.isnan(temp[:, i]))[0]\n",
    "        if len(cc2):\n",
    "            cc3 = np.tile(cc, (len(cc2), 1))\n",
    "            temp2[cc2, i * 32:(i + 1) * 32] = cc3\n",
    "        \n",
    "        needed = np.where(np.isnan(temp2[:, (i + 1) * 32 - 1]))[0]\n",
    "        if len(needed):\n",
    "            temp2[needed, i * 32:(i + 1) * 32] = translate(temp[needed, i])\n",
    "\n",
    "    temp2 = temp2[:, (startandend[0] - start):(startandend[1] - start)]\n",
    "\n",
    "    alleles = np.empty(len(temp2))\n",
    "    alleles[::2] = allele1\n",
    "    alleles[1::2] = allele2\n",
    "    nameset = np.unique(alleles[~np.isnan(alleles)])\n",
    "    \n",
    "    predmat = np.zeros((len(nameset), temp2.shape[1]))\n",
    "    for i, name in enumerate(nameset):\n",
    "        predmat[i, :] = np.mean(temp2[alleles == name, :], axis=0)\n",
    "    \n",
    "    predmatallele1 = np.zeros((len(ourtypes1), temp2.shape[1]))\n",
    "    predmatallele2 = np.zeros((len(ourtypes1), temp2.shape[1]))\n",
    "    \n",
    "    predmatallele1[np.isin(ourtypes1, nameset), :] = predmat[np.searchsorted(nameset, ourtypes1[np.isin(ourtypes1, nameset)]), :]\n",
    "    predmatallele2[np.isin(ourtypes2, nameset), :] = predmat[np.searchsorted(nameset, ourtypes2[np.isin(ourtypes2, nameset)]), :]\n",
    "\n",
    "    obsmatallele1 = temp2[::2, :]\n",
    "    obsmatallele2 = temp2[1::2, :]\n",
    "\n",
    "    dist11 = np.sum(np.abs(obsmatallele1 - predmatallele1) > 0.9, axis=1)\n",
    "    dist12 = np.sum(np.abs(obsmatallele1 - predmatallele2) > 0.9, axis=1)\n",
    "    dist21 = np.sum(np.abs(obsmatallele2 - predmatallele1) > 0.9, axis=1)\n",
    "    dist22 = np.sum(np.abs(obsmatallele2 - predmatallele2) > 0.9, axis=1)\n",
    "\n",
    "    w1 = np.zeros(len(dist11))\n",
    "    w1[dist11 < dist12] = 1\n",
    "    w1[dist11 > dist12] = -1\n",
    "    \n",
    "    w2 = np.zeros(len(dist21))\n",
    "    w2[dist22 < dist21] = 1\n",
    "    w2[dist22 > dist21] = -1\n",
    "\n",
    "    phase1b = dist11 + dist22\n",
    "    phase2b = dist12 + dist21\n",
    "    \n",
    "    # Define phased based on conditions\n",
    "    phased1b = (\n",
    "        (~np.isnan(phase1b) & ~np.isnan(phase2b) & (phase1b < phase2b)) |\n",
    "        (~np.isnan(ourtypes1) & ~np.isnan(ourtypes2) & (ourtypes1 == ourtypes2)) |\n",
    "        (np.isnan(d21) & ~np.isnan(d12) & (d12 - d22 > 2)) |\n",
    "        (np.isnan(d12) & ~np.isnan(d21) & (d21 - d11 > 2))\n",
    "    )\n",
    "\n",
    "    phased2b = (\n",
    "        (~np.isnan(phase1b) & ~np.isnan(phase2b) & (phase1b > phase2b)) |\n",
    "        (np.isnan(d21) & ~np.isnan(d12) & (d22 - d12 > 2)) |\n",
    "        (np.isnan(d12) & ~np.isnan(d21) & (d11 - d21 > 2))\n",
    "    )\n",
    "\n",
    "    # Update phasing\n",
    "    update = ~phased1 & ~phased2\n",
    "    phased1[update] = phased1b[update]\n",
    "    phased2[update] = phased2b[update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
