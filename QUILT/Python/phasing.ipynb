{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import gzip\n",
    "import time\n",
    "import json\n",
    "import secrets\n",
    "import multiprocessing\n",
    "import subprocess\n",
    "import resource\n",
    "import pandas as pd\n",
    "# import sqlite3\n",
    "# from PIL import Image\n",
    "# import matplotlib.pyplot as plt\n",
    "# from plotnine import *\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import statsmodels.api as sm\n",
    "import random\n",
    "# from collections import Counter\n",
    "# import seaborn as sns\n",
    "# import matplotlib.colors as mcolors\n",
    "# from matplotlib.ticker import FuncFormatter\n",
    "import itertools\n",
    "import collections\n",
    "import pyreadr\n",
    "# import patchworklib as pw\n",
    "# sys.path.append('/well/band/users/rbx225/software/lcwgsus/')\n",
    "# import lcwgsus\n",
    "# from lcwgsus.variables import *\n",
    "from warnings import simplefilter\n",
    "simplefilter(action=\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "# Genuine logics changed from the main script were marked by string **X** in the original R files.\n",
    "# Simplicity changes were marked by string **Y**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene = 'DRB1'\n",
    "nucleotides = ['A', 'T', 'C', 'G']\n",
    "ipd_gen_file = '/Users/sus_zhang/Desktop/Suuuuuuuus/misc_data/alignments/' + gene + '_gen.txt'\n",
    "if not os.path.exists(ipd_gen_file):\n",
    "    ipd_gen_file = '/well/band/users/rbx225/recyclable_files/hla/alignments/' + gene + '_gen.txt'\n",
    "\n",
    "hla_gene_information = pd.read_csv('/well/band/users/rbx225/software/QUILT_sus/hla_ancillary_files/hlagenes.txt', sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(ipd_gen_file, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "gDNA_idx = []\n",
    "names = []\n",
    "i = 0\n",
    "while len(gDNA_idx) < 2:\n",
    "    l = lines[i]\n",
    "    if 'gDNA' in l:\n",
    "        gDNA_idx.append(i)\n",
    "    elif l.lstrip(' ').split(' ')[0].startswith(gene + '*'):\n",
    "        name = l.lstrip(' ').split(' ')[0]\n",
    "        names.append(name)\n",
    "    i += 1\n",
    "    \n",
    "first_base = int(lines[gDNA_idx[0]].split(' ')[-1].split('\\n')[0])\n",
    "n_alleles = gDNA_idx[1] - gDNA_idx[0] - 3\n",
    "\n",
    "alleles_dict = {k:'' for k in names}\n",
    "for i, s in enumerate(lines):\n",
    "    r = s.lstrip(' ')\n",
    "    if r.startswith(gene):\n",
    "        r = r.rstrip(' \\n')\n",
    "        name = r.split(' ')[0]\n",
    "        sequence = r.split(' ')[2:]\n",
    "        sequence = ''.join(sequence)\n",
    "        alleles_dict[name] = alleles_dict[name] + sequence\n",
    "    \n",
    "df = pd.DataFrame({key: list(value) for key, value in alleles_dict.items()}).T\n",
    "df = df.drop(columns=df.columns[df.eq('|').all()])\n",
    "df.columns = range(df.shape[1])\n",
    "\n",
    "length = len(df.columns)\n",
    "positions = [first_base]*length\n",
    "\n",
    "df.loc[len(df)] = positions\n",
    "\n",
    "r_idx = len(df) - 1\n",
    "for i in df.columns[1:]:\n",
    "    if df.iloc[0, i] == '.':\n",
    "        df.iloc[r_idx, i] = df.iloc[r_idx, i-1]\n",
    "    else:\n",
    "        if df.iloc[r_idx, i-1] != -1:\n",
    "            df.iloc[r_idx, i] = df.iloc[r_idx, i-1] + 1\n",
    "        else:\n",
    "            df.iloc[r_idx, i] = df.iloc[r_idx, i-1] + 2 # No zero in position. ATG is encoded by 1.\n",
    "df.columns = df.iloc[r_idx]\n",
    "df = df.iloc[:r_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_refdir = '/well/band/users/rbx225/GAMCC/results/hla/imputation/ref_panel/QUILT_ref_files/'\n",
    "auxiliary_dir = '/well/band/users/rbx225/GAMCC/results/hla/imputation/ref_panel/auxiliary_files/' \n",
    "all_haplotypes = pyreadr.read_r(original_refdir + 'quilt.hrc.hla.all.haplotypes.RData') # See if it is possible to remove the 32 bases dependency\n",
    "snpformatalleles = pyreadr.read_r(original_refdir + 'hlaDRB1snpformatalleles.RData')\n",
    "fullallelesfilledin = pyreadr.read_r(original_refdir + 'HLADRB1fullallelesfilledin.RData')\n",
    "full = pyreadr.read_r(original_refdir + 'hlaDRB1full.RData')\n",
    "\n",
    "# Check these two: should contain 1kg HLA types\n",
    "hlatypes = pd.read_csv(auxiliary_dir + '20181129_HLA_types_full_1000_Genomes_Project_panel.txt', sep = '\\t')\n",
    "ref_samples = pd.read_csv(auxiliary_dir + 'oneKG.samples', sep = ' ')\n",
    "ref_samples_removed = ref_samples[~ref_samples['SAMPLE'].isin(hlatypes['Sample ID'].tolist())]\n",
    "samples_to_remove = ref_samples_removed['SAMPLE'].tolist()\n",
    "reference_exclude_samplelist_file = ''\n",
    "\n",
    "if reference_exclude_samplelist_file != '':\n",
    "    samples_to_remove = samples_to_remove + lcwgsus.read_tsv_as_lst(reference_exclude_samplelist_file)\n",
    "# if samples_to_remove is not none, save to \"hlauntyped.exclude.txt\"\n",
    "\n",
    "hlatypes = hlatypes[~hlatypes['Sample ID'].isin(samples_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oldsnpinfo -> prepared_vcf\n",
    "# ss -> variant_alleles\n",
    "# samples -> alleles\n",
    "# qq -> positions\n",
    "knownvarsfiltered = snpformatalleles['knownvarsfiltered'].copy()\n",
    "resmat = snpformatalleles['resmat'].copy()\n",
    "knownvarsfiltered = knownvarsfiltered[knownvarsfiltered[1].isin(nucleotides) & knownvarsfiltered[2].isin(nucleotides)].reset_index(drop = True)\n",
    "\n",
    "ids = \"chr6:\" + knownvarsfiltered[0].astype(str)\n",
    "prepared_vcf = pd.concat([ids, knownvarsfiltered], axis=1)\n",
    "alleles = resmat.index\n",
    "prepared_vcf.columns = [\"id\", \"position\", \"a0\", \"a1\"]\n",
    "prepared_vcf['position']  = prepared_vcf['position'].astype(int)\n",
    "\n",
    "ourpos = fullallelesfilledin['ourpos']['ourpos'].tolist()\n",
    "fullalleles = fullallelesfilledin['fullalleles'].copy()\n",
    "\n",
    "retained_index = []\n",
    "variant_alleles = pd.DataFrame(index=fullalleles.index)\n",
    "\n",
    "for i in range(len(prepared_vcf)):\n",
    "    _, pos, ref, alt = prepared_vcf.iloc[i,:]\n",
    "    index = np.where(ourpos == pos)[0]\n",
    "    if index.size != 0:\n",
    "        index = index[0]\n",
    "        retained_index.append(index)\n",
    "        variant_alleles[variant_alleles.shape[1]] = fullalleles[index].map({ref: '0', alt: '1'}).fillna(fullalleles[index])\n",
    "    else:\n",
    "        prepared_vcf.iloc[i,2] = 'tbr' # tbr: To be removed\n",
    "\n",
    "prepared_vcf = prepared_vcf[prepared_vcf['a0'] != 'tbr'].reset_index(drop = True)\n",
    "variant_alleles = variant_alleles.T\n",
    "\n",
    "positions = prepared_vcf['position'].astype(float).values  # Convert position to double\n",
    "\n",
    "zz = np.zeros(variant_alleles.shape[0]) \n",
    "for i in range(len(zz)):\n",
    "    zz[i] = np.sum(ourpos == positions[i])\n",
    "zz2 = np.zeros_like(zz)\n",
    "zz2 = np.sum(~((variant_alleles == '0') | (variant_alleles == '1')), axis=1)\n",
    "zz3 = np.zeros_like(zz)\n",
    "for i in range(len(zz3)):\n",
    "    zz3[i] = np.sum(positions == positions[i])\n",
    "zz4 = np.sum(~((variant_alleles == '0') | (variant_alleles == '1') | (variant_alleles == \".\")), axis=1)\n",
    "filter_conditions = (zz3 == 1) & (zz == 1) & (zz2 < variant_alleles.shape[1] * 0.1) & (zz2 < 0.5 * np.sum(variant_alleles == '1', axis=1)) & (zz4 == 0)\n",
    "\n",
    "# keep sites uniquely mapping, not overlapping another SNP, with at most 10% gaps and 2-fold more non-ancestral than gaps\n",
    "snpinfo = prepared_vcf.loc[filter_conditions, :].reset_index(drop = True)\n",
    "variant_alleles = variant_alleles.loc[filter_conditions, :]\n",
    "variant_alleles[variant_alleles == \".\"] = 0 # indels set to 0\n",
    "haps = variant_alleles.copy().reset_index(drop = True)\n",
    "# haps is `n_variant \\times n_alleles` df, with each entry as 0/1 representing ref/alt from the fake vcf \"prepared_vcf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.array(alleles).reshape(-1, 1)\n",
    "c1 = [s[0].split('*') for s in samples]\n",
    "vv = np.empty((samples.shape[0], 3), dtype=object)\n",
    "ww = [c[1] for c in c1]\n",
    "for i in range(len(c1)):\n",
    "    vv[i, 0] = c1[i][0]\n",
    "c1 = [w.split(':') for w in ww]\n",
    "for i in range(len(c1)):\n",
    "    vv[i, 1] = c1[i][0]\n",
    "    vv[i, 2] = c1[i][1]\n",
    "fourdigit = [f\"{vv[i, 1]}:{vv[i, 2]}\" for i in range(len(vv))]\n",
    "ufourdigit = np.unique(fourdigit)\n",
    "# Convert reference alleles to 4-digit resolution. vv and ww are just intermediate variables of no use\n",
    "# ufourdigit is unique 4-digit alleles\n",
    "\n",
    "haps = np.array(haps, dtype=float)\n",
    "newhaps = np.empty((haps.shape[0], len(ufourdigit)))\n",
    "for i in range(newhaps.shape[1]):\n",
    "    tt = np.where(np.array(fourdigit) == ufourdigit[i])[0]\n",
    "    if len(tt) < 2:\n",
    "        tt = np.concatenate((tt, tt))\n",
    "    newhaps[:, i] = np.mean(haps[:, tt], axis=1)\n",
    "newhaps = pd.DataFrame(newhaps, columns = ufourdigit)\n",
    "# newhaps is the result after averaging 4 digit resolution\n",
    "    \n",
    "# pos is from the QUILT_prepare_ref utility. It seems to record biallelic SNP variant information\n",
    "pos = all_haplotypes['pos']\n",
    "\n",
    "cols = snpinfo.columns.tolist()\n",
    "full_vcf = pos.copy()[['POS', 'REF', 'ALT']]\n",
    "full_vcf.columns = cols[1:]\n",
    "same = pd.merge(full_vcf, snpinfo, on = cols[1:], how = 'inner')\n",
    "\n",
    "snpinfo_flipped = snpinfo.iloc[:,[0, 1, 3, 2]]\n",
    "snpinfo_flipped.columns = cols\n",
    "flipped = pd.merge(full_vcf, snpinfo_flipped, on = cols[1:], how = 'inner')\n",
    "\n",
    "newhaps2 = newhaps.loc[newhaps.index.isin(snpinfo[snpinfo['position'].isin(same['position'])].index)]\n",
    "if len(flipped) != 0:\n",
    "    newhaps2_flipped = 1 - newhaps.loc[newhaps.index.isin(snpinfo[snpinfo['position'].isin(flipped['position'])].index)]\n",
    "    newhaps2 = pd.concat([newhaps2, newhaps2_flipped])\n",
    "newhaps2 = newhaps2.reset_index(drop = True)\n",
    "# newhaps2 further filtered out variants that are not observed in the 'pos' dataframe\n",
    "# This step really filters out a plethora of variants.. Is this really necessary? \n",
    "# Is it possible to circumvent that step such that we dont utilise any information from the `QUILT` output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(matchlist):\n",
    "    qq = np.array(matchlist)\n",
    "    signs = np.zeros(len(matchlist), dtype=int)\n",
    "    \n",
    "    signs[qq < 0] = 1\n",
    "    qq[signs == 1] += 2**31\n",
    "    \n",
    "    res = np.zeros((len(matchlist), 0), dtype=float)\n",
    "    \n",
    "    for i in range(31):\n",
    "        tempres = qq % 2\n",
    "        qq = (qq - tempres) // 2\n",
    "        res = np.hstack((res, tempres.reshape(-1, 1)))\n",
    "    \n",
    "    res = np.hstack((res, signs.reshape(-1, 1)))\n",
    "    \n",
    "    res[res == 1] = 0.999\n",
    "    res[res == 0] = 0.001\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Find the relevant columns using a pattern\n",
    "region_pattern = f\"HLA.{region}.\"\n",
    "cols = [i for i, col in enumerate(hlatypes2.columns) if re.search(region_pattern, col)]\n",
    "\n",
    "# Step 2: Extract and clean 'ourtypes1'\n",
    "ourtypes1 = hlatypes2.iloc[:, cols[0]].astype(str).tolist()\n",
    "ourtypes1 = [re.sub(r\"\\*\", \"\", t) for t in ourtypes1]\n",
    "\n",
    "# Step 3: If multiple types exist, assign the lowest number\n",
    "vv = [t.split('/') for t in ourtypes1]\n",
    "for i in range(len(ourtypes1)):\n",
    "    ourtypes1[i] = vv[i][0]\n",
    "\n",
    "# Step 4: Extract and clean 'ourtypes2'\n",
    "ourtypes2 = hlatypes2.iloc[:, cols[1]].astype(str).tolist()\n",
    "ourtypes2 = [re.sub(r\"\\*\", \"\", t) for t in ourtypes2]\n",
    "\n",
    "# Step 5: Assign the lowest number if there are multiple possible types\n",
    "vv = [t.split('/') for t in ourtypes2]\n",
    "for i in range(len(ourtypes2)):\n",
    "    ourtypes2[i] = vv[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Define start and end ranges\n",
    "startandend = range(locs[0], locs[-1] + 1)\n",
    "\n",
    "start32 = (startandend[0] // 32) + 1  # per-chunk start\n",
    "end32 = (startandend[-1] // 32) + 1\n",
    "\n",
    "start = (start32 - 1) * 32 + 1\n",
    "end = end32 * 32\n",
    "\n",
    "# Step 2: Slice distinctHapsIE, distinctHapsB, and rhb_t matrices\n",
    "tempIE = distinctHapsIE[:, start-1:end]\n",
    "tempB = distinctHapsB[:, start32-1:end32]\n",
    "\n",
    "# Assuming rhb_t is a matrix of 1s or -1s for strand information\n",
    "temp = rhb_t[:, start32-1:end32]\n",
    "\n",
    "# Step 3: Initialize temp2 matrix\n",
    "temp2 = np.empty((temp.shape[0], tempIE.shape[1]))\n",
    "\n",
    "# Step 4: Define 'cc' vector (for NA interpretation)\n",
    "cc = np.concatenate((np.full(31, 0.001), [0.999]))\n",
    "\n",
    "# Step 5: Iterate over columns of temp\n",
    "for i in range(temp.shape[1]):\n",
    "    start_col = (i * 32)\n",
    "    end_col = (i + 1) * 32\n",
    "\n",
    "    # Match temp with tempB and fill temp2\n",
    "    temp2[:, start_col:end_col] = tempIE[np.searchsorted(tempB[:, i], temp[:, i]), start_col:end_col]\n",
    "\n",
    "    # Handle NA (None) cases in temp\n",
    "    cc2 = np.where(np.isnan(temp[:, i]))[0]\n",
    "    if len(cc2) > 0:\n",
    "        cc3 = np.tile(cc, (len(cc2), 1))\n",
    "        temp2[cc2, start_col:end_col] = cc3\n",
    "\n",
    "    # Final step: Handle NA in temp2 for the final column of the block\n",
    "    needed = np.where(np.isnan(temp2[:, end_col-1]))[0]\n",
    "    if len(needed) > 0:\n",
    "        temp2[needed, start_col:end_col] = translate(temp[needed, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Step 1: Adjust `locs` and extract `hrchapstomatch`\n",
    "newlocs = locs - start + 1\n",
    "hrchapstomatch = temp2[:, newlocs-1]  # Adjust for Python's zero-indexing\n",
    "\n",
    "# Step 2: Split into first and second alleles\n",
    "hrcfirstalleles = hrchapstomatch[::2, :]\n",
    "hrcsecondalleles = hrchapstomatch[1::2, :]\n",
    "\n",
    "# Step 3: Initialize temp and handle `ourtypes1`\n",
    "temp = np.full(len(ourtypes1), -1)\n",
    "temp[np.isin(ourtypes1, newhaps2.columns)] = 1\n",
    "\n",
    "# Step 4: Predict first alleles\n",
    "predfirstalleles = np.empty((len(temp), newhaps2.shape[0]))\n",
    "predfirstalleles[temp == 1, :] = newhaps2.loc[:, ourtypes1[temp == 1]].T\n",
    "\n",
    "# Step 5: Handle `ourtypes2`\n",
    "temp = np.full(len(ourtypes2), -1)\n",
    "temp[np.isin(ourtypes2, newhaps2.columns)] = 1\n",
    "\n",
    "# Step 6: Predict second alleles\n",
    "predsecondalleles = np.empty((len(temp), newhaps2.shape[0]))\n",
    "predsecondalleles[temp == 1, :] = newhaps2.loc[:, ourtypes2[temp == 1]].T\n",
    "\n",
    "# Step 7: Compute distances\n",
    "dist11 = np.sum(np.abs(hrcfirstalleles - predfirstalleles), axis=1)\n",
    "dist12 = np.sum(np.abs(hrcfirstalleles - predsecondalleles), axis=1)\n",
    "dist21 = np.sum(np.abs(hrcsecondalleles - predfirstalleles), axis=1)\n",
    "dist22 = np.sum(np.abs(hrcsecondalleles - predsecondalleles), axis=1)\n",
    "\n",
    "# Step 8: Perform alignment and assign `w`\n",
    "w = np.zeros(len(dist11))\n",
    "w[dist11 > dist12] = 1\n",
    "w[dist11 < dist12] = -1\n",
    "w1 = w\n",
    "\n",
    "w = np.zeros(len(dist21))\n",
    "w[dist22 < dist21] = 1\n",
    "w[dist22 > dist21] = -1\n",
    "w2 = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Step 1: Calculate observed and predicted genotypes\n",
    "obsgen = hrcfirstalleles + hrcsecondalleles\n",
    "predgen = predfirstalleles + predsecondalleles\n",
    "\n",
    "# Step 2: Calculate correlations column-wise\n",
    "corr = np.empty(obsgen.shape[1])\n",
    "for i in range(obsgen.shape[1]):\n",
    "    with np.errstate(invalid='ignore'):\n",
    "        corr[i] = pearsonr(obsgen[:, i], predgen[:, i])[0] if not np.all(np.isnan(predgen[:, i])) else np.nan\n",
    "\n",
    "# Step 3: Filter columns with correlation > 0.8 and non-NA values\n",
    "valid_corr_indices = np.where((corr > 0.8) & (~np.isnan(corr)))[0]\n",
    "\n",
    "# Step 4: Compute distances for the filtered columns\n",
    "dist11 = np.sum(np.abs(hrcfirstalleles[:, valid_corr_indices] - predfirstalleles[:, valid_corr_indices]), axis=1)\n",
    "dist12 = np.sum(np.abs(hrcfirstalleles[:, valid_corr_indices] - predsecondalleles[:, valid_corr_indices]), axis=1)\n",
    "dist21 = np.sum(np.abs(hrcsecondalleles[:, valid_corr_indices] - predfirstalleles[:, valid_corr_indices]), axis=1)\n",
    "dist22 = np.sum(np.abs(hrcsecondalleles[:, valid_corr_indices] - predsecondalleles[:, valid_corr_indices]), axis=1)\n",
    "\n",
    "# Step 5: Calculate w1 and w2 based on distances\n",
    "w = np.zeros(len(dist11))\n",
    "w[dist11 < dist12] = 1\n",
    "w[dist11 > dist12] = -1\n",
    "w1 = w\n",
    "\n",
    "w = np.zeros(len(dist21))\n",
    "w[dist22 < dist21] = 1\n",
    "w[dist22 > dist21] = -1\n",
    "w2 = w\n",
    "\n",
    "# Print the contingency table of w1 and w2\n",
    "# We can use numpy's `np.histogram2d()` to create a table equivalent to `table()`\n",
    "w1_w2_table = np.histogram2d(w1, w2, bins=[[-1, 0, 1], [-1, 0, 1]])[0]\n",
    "\n",
    "# Step 6: Calculate phasing metrics\n",
    "phase1 = dist11 + dist22\n",
    "phase2 = dist12 + dist21\n",
    "\n",
    "# Step 7: Criteria for phasing\n",
    "d11 = dist11\n",
    "d21 = dist21\n",
    "d12 = dist12\n",
    "d22 = dist22\n",
    "\n",
    "phased = (\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 < 4) & (phase2 > 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 > 4) & (phase2 < 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 - phase2 > 2)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase2 - phase1 > 2)) |\n",
    "    (~np.isnan(ourtypes1) & ~np.isnan(ourtypes2) & (ourtypes1 == ourtypes2)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d22 - d12 > 2)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d12 - d22 > 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d11 - d21 > 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d21 - d11 > 2))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Define phased1 and phased2 based on conditions\n",
    "phased1 = (\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 < 4) & (phase2 > 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase2 - phase1 > 2) & (phase1 < 4)) |\n",
    "    (~np.isnan(ourtypes1) & ~np.isnan(ourtypes2) & (ourtypes1 == ourtypes2)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d12 - d22 > 2) & (d22 < 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d21 - d11 > 2) & (d11 < 2))\n",
    ")\n",
    "\n",
    "phased2 = (\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 > 4) & (phase2 < 4)) |\n",
    "    (~np.isnan(phase1) & ~np.isnan(phase2) & (phase1 - phase2 > 2) & (phase2 < 4)) |\n",
    "    (np.isnan(d21) & ~np.isnan(d12) & (d22 - d12 > 2) & (d12 < 2)) |\n",
    "    (np.isnan(d12) & ~np.isnan(d21) & (d11 - d21 > 2) & (d21 < 2))\n",
    ")\n",
    "\n",
    "# Step 2: Assign alleles based on phased1 and phased2\n",
    "allele1 = np.full(len(ourtypes1), np.nan)\n",
    "allele2 = np.full(len(ourtypes1), np.nan)\n",
    "allele1[phased1] = ourtypes1[phased1]\n",
    "allele2[phased1] = ourtypes2[phased1]\n",
    "allele1[phased2] = ourtypes2[phased2]\n",
    "allele2[phased2] = ourtypes1[phased2]\n",
    "\n",
    "# Step 3: Iterate over extension sizes and refine phasing\n",
    "phased1old = phased1.copy()\n",
    "phased2old = phased2.copy()\n",
    "oldphase1 = phase1.copy()\n",
    "oldphase2 = phase2.copy()\n",
    "\n",
    "for extension in range(50, 1001, 50):\n",
    "    plt.scatter(oldphase1, oldphase2, c=1 + phased1.astype(int) + 2 * phased2.astype(int))\n",
    "    plt.show()\n",
    "\n",
    "    startandend = np.array([locs.min(), locs.max()])\n",
    "    startandend[0] -= extension\n",
    "    startandend[1] += extension\n",
    "    start32 = startandend[0] // 32 + 1\n",
    "    end32 = startandend[1] // 32 + 1\n",
    "    start = (start32 - 1) * 32 + 1\n",
    "    end = end32 * 32\n",
    "\n",
    "    tempIE = distinctHapsIE[:, start:end]\n",
    "    tempB = distinctHapsB[:, start32:end32]\n",
    "    temp = rhb_t[:, start32:end32]\n",
    "    temp2 = np.empty((temp.shape[0], tempIE.shape[1]))\n",
    "\n",
    "    cc = np.array([0.001] * 31 + [0.999])\n",
    "    \n",
    "    for i in range(temp.shape[1]):\n",
    "        temp2[:, i * 32:(i + 1) * 32] = tempIE[np.searchsorted(tempB[:, i], temp[:, i]), i * 32:(i + 1) * 32]\n",
    "        \n",
    "        cc2 = np.where(np.isnan(temp[:, i]))[0]\n",
    "        if len(cc2):\n",
    "            cc3 = np.tile(cc, (len(cc2), 1))\n",
    "            temp2[cc2, i * 32:(i + 1) * 32] = cc3\n",
    "        \n",
    "        needed = np.where(np.isnan(temp2[:, (i + 1) * 32 - 1]))[0]\n",
    "        if len(needed):\n",
    "            temp2[needed, i * 32:(i + 1) * 32] = translate(temp[needed, i])\n",
    "\n",
    "    temp2 = temp2[:, (startandend[0] - start):(startandend[1] - start)]\n",
    "\n",
    "    alleles = np.empty(len(temp2))\n",
    "    alleles[::2] = allele1\n",
    "    alleles[1::2] = allele2\n",
    "    nameset = np.unique(alleles[~np.isnan(alleles)])\n",
    "    \n",
    "    predmat = np.zeros((len(nameset), temp2.shape[1]))\n",
    "    for i, name in enumerate(nameset):\n",
    "        predmat[i, :] = np.mean(temp2[alleles == name, :], axis=0)\n",
    "    \n",
    "    predmatallele1 = np.zeros((len(ourtypes1), temp2.shape[1]))\n",
    "    predmatallele2 = np.zeros((len(ourtypes1), temp2.shape[1]))\n",
    "    \n",
    "    predmatallele1[np.isin(ourtypes1, nameset), :] = predmat[np.searchsorted(nameset, ourtypes1[np.isin(ourtypes1, nameset)]), :]\n",
    "    predmatallele2[np.isin(ourtypes2, nameset), :] = predmat[np.searchsorted(nameset, ourtypes2[np.isin(ourtypes2, nameset)]), :]\n",
    "\n",
    "    obsmatallele1 = temp2[::2, :]\n",
    "    obsmatallele2 = temp2[1::2, :]\n",
    "\n",
    "    dist11 = np.sum(np.abs(obsmatallele1 - predmatallele1) > 0.9, axis=1)\n",
    "    dist12 = np.sum(np.abs(obsmatallele1 - predmatallele2) > 0.9, axis=1)\n",
    "    dist21 = np.sum(np.abs(obsmatallele2 - predmatallele1) > 0.9, axis=1)\n",
    "    dist22 = np.sum(np.abs(obsmatallele2 - predmatallele2) > 0.9, axis=1)\n",
    "\n",
    "    w1 = np.zeros(len(dist11))\n",
    "    w1[dist11 < dist12] = 1\n",
    "    w1[dist11 > dist12] = -1\n",
    "    \n",
    "    w2 = np.zeros(len(dist21))\n",
    "    w2[dist22 < dist21] = 1\n",
    "    w2[dist22 > dist21] = -1\n",
    "\n",
    "    phase1b = dist11 + dist22\n",
    "    phase2b = dist12 + dist21\n",
    "    \n",
    "    # Define phased based on conditions\n",
    "    phased1b = (\n",
    "        (~np.isnan(phase1b) & ~np.isnan(phase2b) & (phase1b < phase2b)) |\n",
    "        (~np.isnan(ourtypes1) & ~np.isnan(ourtypes2) & (ourtypes1 == ourtypes2)) |\n",
    "        (np.isnan(d21) & ~np.isnan(d12) & (d12 - d22 > 2)) |\n",
    "        (np.isnan(d12) & ~np.isnan(d21) & (d21 - d11 > 2))\n",
    "    )\n",
    "\n",
    "    phased2b = (\n",
    "        (~np.isnan(phase1b) & ~np.isnan(phase2b) & (phase1b > phase2b)) |\n",
    "        (np.isnan(d21) & ~np.isnan(d12) & (d22 - d12 > 2)) |\n",
    "        (np.isnan(d12) & ~np.isnan(d21) & (d11 - d21 > 2))\n",
    "    )\n",
    "\n",
    "    # Update phasing\n",
    "    update = ~phased1 & ~phased2\n",
    "    phased1[update] = phased1b[update]\n",
    "    phased2[update] = phased2b[update]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
